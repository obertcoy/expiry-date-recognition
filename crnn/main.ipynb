{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.backend import ctc_decode, get_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../dataset/Date-Synth/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_PATH = '../dataset/Date-Synth/annotations.json'\n",
    "\n",
    "with open(ANNOTATIONS_PATH, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def get_y_from_json(file: str) -> str:\n",
    "    return \" \".join(annotation['transcription'] for annotation in data[file]['ann'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 10 2016'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_y_from_json('00001.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:  59%|█████▉    | 11840/20000 [00:04<00:03, 2402.33it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "target_size = (32, 100)\n",
    "\n",
    "n = 20000\n",
    "\n",
    "for idx, file in enumerate(tqdm(os.listdir(PATH)[:n], total=n, desc=\"Loading images\")):\n",
    "\n",
    "    file_path = os.path.join(PATH, file)\n",
    "\n",
    "    image = tf.keras.preprocessing.image.load_img(file_path, color_mode='grayscale', target_size=target_size)\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "\n",
    "    label = get_y_from_json(file)\n",
    "\n",
    "    if label != \" \":\n",
    "        X.append(image)\n",
    "        Y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "    \n",
    "plt.imshow(X[100], cmap='gray')\n",
    "plt.title(Y[100])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (11840, 32, 100, 1) | Y shape: (11840,)\n",
      "X_normalized[0]: [[[0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  ...\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]]\n",
      "\n",
      " [[0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  ...\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]]\n",
      "\n",
      " [[0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  ...\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  ...\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]]\n",
      "\n",
      " [[0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  ...\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]]\n",
      "\n",
      " [[0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  ...\n",
      "  [0.98039216]\n",
      "  [0.98039216]\n",
      "  [0.98039216]]] | Y[0]: 21 09 2020\n"
     ]
    }
   ],
   "source": [
    "# Normalized\n",
    "\n",
    "X_normalized = X / 255.0\n",
    "\n",
    "print(f'X shape: {X_normalized.shape} | Y shape: {Y.shape}')\n",
    "print(f'X_normalized[0]: {X_normalized[0]} | Y[0]: {Y[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 09 2020\n"
     ]
    }
   ],
   "source": [
    "# Encode\n",
    "\n",
    "char_list = string.ascii_letters + string.digits + ' ' # Digits because only for expiration date\n",
    "\n",
    "index_to_char = {idx: character for idx, character in enumerate(char_list)}\n",
    "\n",
    "def encode_label(label: str) -> list:\n",
    "    \n",
    "    encoded = []\n",
    "\n",
    "    for char in label:\n",
    "        try:\n",
    "            encoded.append(char_list.index(char))\n",
    "        except:\n",
    "            print(f'Not found in char_list: {char}')\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def decode_label(encoded) -> list:\n",
    "        \n",
    "    decoded = []\n",
    "\n",
    "    for char_idx in encoded:\n",
    "        \n",
    "        if int(char_idx) != -1 and int(char_idx) != len(char_list): # -1 -> _ in CTC and len(char_list) -> pad\n",
    "            \n",
    "            decoded.append(index_to_char[char_idx])\n",
    "                \n",
    "    return decoded\n",
    "    \n",
    "Y_encoded = [encode_label(label) for label in Y]\n",
    "\n",
    "print(''.join([index_to_char[encode_idx] for encode_idx in Y_encoded[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max sequence len\n",
    "\n",
    "max_len = max(len(seq) for seq in Y_encoded)\n",
    "\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequence\n",
    "\n",
    "Y_padded = tf.keras.preprocessing.sequence.pad_sequences(Y_encoded, maxlen= max_len, padding='post', value=len(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y_padded, test_size= 0.2, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size= 0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 100, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input shape\n",
    "\n",
    "input_shape = X[0].shape\n",
    "\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapToSequenceLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "\n",
    "        # Input = (batch_size, height, width, channels) => Feature Map\n",
    "\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        height = tf.shape(inputs)[1]\n",
    "        width = tf.shape(inputs)[2]\n",
    "        channels = tf.shape(inputs)[3]\n",
    "\n",
    "\n",
    "        # Output = (batch_size, width, height * channels) => sequence per columns\n",
    "\n",
    "        outputs = tf.reshape(inputs, (batch_size, width, height * channels))\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLoss(tf.keras.losses.Loss):\n",
    "\n",
    "    def __init__(self, name: str = 'CTCLoss') -> None:\n",
    "\n",
    "        super(CTCLoss, self).__init__()\n",
    "        self.name = name\n",
    "        self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
    "\n",
    "    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor, sample_weight= None) -> tf.Tensor:\n",
    "\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype='int64')\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype='int64')\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype='int64')\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 32, 100, 64)       640       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 16, 50, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 16, 50, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 8, 25, 128)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 8, 25, 256)        295168    \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 8, 25, 256)        590080    \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 4, 25, 256)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 4, 25, 512)        1180160   \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 4, 25, 512)       2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 4, 25, 512)        2359808   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 4, 25, 512)       2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 2, 25, 512)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 1, 24, 512)        1049088   \n",
      "                                                                 \n",
      " tf.compat.v1.shape_5 (TFOpL  (4,)                     0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " tf.compat.v1.shape_7 (TFOpL  (4,)                     0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " tf.compat.v1.shape_4 (TFOpL  (4,)                     0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " tf.compat.v1.shape_6 (TFOpL  (4,)                     0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " tf.__operators__.getitem_5   ()                       0         \n",
      " (SlicingOpLambda)                                               \n",
      "                                                                 \n",
      " tf.__operators__.getitem_7   ()                       0         \n",
      " (SlicingOpLambda)                                               \n",
      "                                                                 \n",
      " tf.__operators__.getitem_4   ()                       0         \n",
      " (SlicingOpLambda)                                               \n",
      "                                                                 \n",
      " tf.__operators__.getitem_6   ()                       0         \n",
      " (SlicingOpLambda)                                               \n",
      "                                                                 \n",
      " tf.math.multiply_1 (TFOpLam  ()                       0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " tf.reshape_1 (TFOpLambda)   (None, 24, 512)           0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 24, 512)          1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 24, 512)          1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24, 64)            32832     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,735,552\n",
      "Trainable params: 8,733,504\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=input_shape),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 1)),\n",
    "    tf.keras.layers.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 1)),\n",
    "    tf.keras.layers.Conv2D(512, kernel_size=(2, 2), padding='valid', activation='relu'),\n",
    "    MapToSequenceLayer(),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
    "    tf.keras.layers.Dense(len(char_list) + 1, activation='softmax')\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52 54 62 29 30 28 62 54 59 63 63]\n",
      "64\n",
      "(None, 24, 64)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0])\n",
    "print(len(char_list) + 1)\n",
    "print(model.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "592/592 [==============================] - 32s 43ms/step - loss: 36.9857 - val_loss: 29.6984\n",
      "Epoch 2/100\n",
      "592/592 [==============================] - 24s 41ms/step - loss: 28.5880 - val_loss: 27.8189\n",
      "Epoch 3/100\n",
      "332/592 [===============>..............] - ETA: 10s - loss: 27.4866"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdadelta(), loss\u001b[38;5;241m=\u001b[39m CTCLoss())\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "\n",
    "model.compile(optimizer= tf.keras.optimizers.Adadelta(), loss= CTCLoss())\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          validation_data= (X_val, Y_val), \n",
    "          batch_size= BATCH_SIZE, \n",
    "          epochs= EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 2s 31ms/step\n",
      "(1184,)\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "ctc_input_length = np.ones(Y_pred.shape[0]) * Y_pred.shape[1]\n",
    "\n",
    "print(ctc_input_length.shape)\n",
    "\n",
    "output = get_value(ctc_decode(Y_pred, input_length= ctc_input_length, greedy= True)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54, 62, 53, 58, 62, 53, 59, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 60, 62, 35, 46, 39, 62, 53, 59, 63, 63])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character-level accuracy: 0.3045\n",
      "Average exact match accuracy: 0.0008445945945945946\n"
     ]
    }
   ],
   "source": [
    "def character_accuracy(pred_text, truth_text):\n",
    "    correct_chars = sum([1 for p, t in zip(pred_text, truth_text) if p == t])\n",
    "    return correct_chars / len(truth_text) if len(truth_text) > 0 else 0.0\n",
    "\n",
    "def exact_match_accuracy(pred_text, truth_text):\n",
    "    return 1 if pred_text == truth_text else 0\n",
    "\n",
    "correct_char_count = 0\n",
    "exact_match_count = 0\n",
    "total_samples = Y_pred.shape[0]  # Checking the first 5 samples\n",
    "\n",
    "for idx in range(total_samples):\n",
    "    pred_text = ''.join([char for char in decode_label(output[idx])])\n",
    "    truth_text = ''.join([char for char in decode_label(Y_test[idx])])\n",
    "\n",
    "    char_acc = character_accuracy(pred_text, truth_text)\n",
    "    exact_acc = exact_match_accuracy(pred_text, truth_text)\n",
    "    \n",
    "    # print(f\"Pred = {pred_text} | Truth: {truth_text}\")\n",
    "    # print(f\"Character-level accuracy: {char_acc:.4f}\")\n",
    "    # print(f\"Exact match accuracy: {exact_acc}\")\n",
    "    \n",
    "    correct_char_count += char_acc\n",
    "    exact_match_count += exact_acc\n",
    "\n",
    "avg_char_accuracy = correct_char_count / total_samples\n",
    "avg_exact_accuracy = exact_match_count / total_samples\n",
    "\n",
    "print(f\"Average character-level accuracy: {avg_char_accuracy:.4f}\")\n",
    "print(f\"Average exact match accuracy: {avg_exact_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
